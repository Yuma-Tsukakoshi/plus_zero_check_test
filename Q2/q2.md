# (1)ロジスティク回帰以外に考えられるアルゴリズムとその概要

- 決定木：  
概要： 教師あり学習  
決定木とは木構造を用いて分類や回帰を行う機械学習の手法の一つである。
木構造でデータを上から各クラスに分類していくので、分析結果の解釈が容易になるというメリットがある。

- k近傍法：  
概要: 教師あり学習  
クラス判別用の手法である。  
学習データをベクトル空間上にプロットしておき、未知のデータが得られたら、そこから距離が近い順に任意のK個を取得し、多数決でデータが属するクラスを推定するもの。

- SVM：  
概要: 教師あり学習  
SVMは分類アルゴリズムの一つで、分類線を決める際にその分類線に最も近い点、つまり、サポートベクターを定める。  
この各クラスのサポートベクター間の距離＝マージンを最大化させるように分類線を定めるもの。



# (3.2) 
用いた確率分布は、標準正規分布である。
パラメータは、平均と標準偏差である。

# (3.3)
targetには、setosa, versicolor, virginicaの品種がラベルつけされている。  
seedを固定しコードを何度か実行したところ、yの各ラベルに対応するy_predの値は似通った値となっていた。  
つまり、yの値を各ラベルで分類する際の閾値に置換し、その値よりも大きいか小さいかで比較することができると考えた。  
閾値の設定方法は、品種が50個のデータごとに別れているので、50個ずつのy_predの平均値を取ることで設定することにした。  
判定結果として0(False)と1(True)の二値のみにした配列をy_resultと定義し精度を算出した。モデルの精度としては、54％の値を示した。

# (4)
ロジスティック回帰に良く用いられる損失関数は交差エントロピー誤差関数である。以下に数式を示す。  
今回の問題では、p(x_i)の値はy_predに値する。

```math
\begin{align}

L(θ) &= \frac{1}{m} \sum_{i=1}^{m} Cost(p(x_i), y_i)\
&= - \frac{1}{m} \sum_{i=1}^{m} y_i\log(p(x_i)) + (1 - y_i)\log(1-p(x_i))

\end{align}
```
bを最適化する方法としては、勾配降下法が考えられる。  
勾配降下法は、関数の勾配（傾き）を計算して、パラメータを少しずつ移動させていく。  
このようにして関数の値が減少する方向に進んでいくことで、最小値に近づけて行く。

# (5)
データをすべて用いると計算コストが増加してしまったり、訓練データに過度に適合することで過学習が起きる可能性が出てくる。  
上記の弊害を避ける方法は以下の通りである。  
ミニバッチ勾配降下法: ミニバッチ勾配降下法では、データ全体ではなくランダムなサンプルの一部（ミニバッチ）を使用して勾配を計算する。これにより、計算コストを削減し、過学習のリスクを低減できる。  

正則化: L1やL2正則化などの手法を用いて、正則化項を追加することでモデルの複雑さを制御することができる。これにより、過学習のリスクを軽減し、モデルの汎化性能を向上させることができる。  

早期終了: トレーニングを途中で終了させる方法。検証データの性能が改善しなくなったときにトレーニングを終了させることで、過学習を避けることができる。
